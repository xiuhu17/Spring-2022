\documentclass{article}
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem*{proposition}{Proposition}
\newtheorem*{definition}{Definition}
\newtheorem*{corrolary}{Corrolary}
\newtheorem*{consider}{Consider}
\newtheorem*{theorem}{Theorem}
\newtheorem*{suppose}{Suppose}
\newtheorem*{notice}{Notice}
\newtheorem*{define}{Define}
\newtheorem*{denote}{Denote}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}
\newtheorem*{proof}{Proof}
\newtheorem*{case}{Case}
%\begin{equation*} \end{equation*}%
%\begin{equation} \end{equation}%
%\begin{split} \end{split}%
%\begin{cases} \end{cases}%
%\subsubsection*{(a)}%
%\subsection*{(a)}%
%\textbf{}%
%\textit{}%
%\noindent%
%\paragraph{}%
%\page{}%
%\\%
%$\langle \rangle$%
%$\| \|$%
% \begin{equation*}
%     \begin{split}
%         & (\lfloor \frac{t}{d_3} \rfloor - 2p) + f((t \mod d_3) + 2 d_3 p, 2) \\
%         \rightarrow & (\lfloor \frac{t}{d_3} \rfloor - 2p) 
%         + \lfloor \frac{(t \mod d_3) + 2 d_3 p}{d_2} \rfloor 
%         + f(((t \mod d_3) + 2 d_3 p) \mod d_2, 1) \\
%         = & (\lfloor \frac{t}{25} \rfloor - 2p) 
%         + \lfloor \frac{(t \mod 25) + 2 \cdot 25 \cdot p}{10} \rfloor 
%         + f(((t \mod 25) + 2 \cdot 25 \cdot p) \mod 10, 1) \\
%         = & (\lfloor \frac{t}{25} \rfloor - 2p) 
%         + \lfloor \frac{t \mod 25}{10} \rfloor + 5 p 
%         + f((t \mod 25) \mod 10, 1) \\
%         = & \lfloor \frac{t}{25} \rfloor
%         + \lfloor \frac{t \mod 25}{10} \rfloor + 3 p 
%         + f((t \mod 25) \mod 10, 1) 
%     \end{split}
%     \end{equation*}

% \begin{equation}
%     \begin{split}
%     (a + b)^4
%       &= (a + b)^2 (a + b)^2     \\
%       &= (a^2 + 2ab + b^2)
%         (a^2 + 2ab + b^2)       \\
%       &= a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + b^4
%     \end{split}
%    \end{equation}

\geometry{a4paper}
\title{Homework11}
\author{Zhihao Wang} 
\date{04/22/2022}
\begin{document}
\maketitle 

\subsection*{4.5.4}
\subsubsection*{(a)}
\begin{denote}
    The $ \left(\begin{bmatrix}
        1 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        1 \\ 1
    \end{bmatrix} \right) $ as $B_v$
\end{denote}
The $ith$ column of $[R]_{B_v}$ is $[Rv_i]_{B_v}$.
\begin{equation*}
    \begin{split}
        &[Rv_1]_{B_v} = \begin{bmatrix}
            1 \\ 0
        \end{bmatrix}_{B_v} = \begin{bmatrix}
            1 \\ 0
        \end{bmatrix} \\
        &[Rv_2]_{B_v} = \begin{bmatrix}
            1 \\ -1
        \end{bmatrix}_{B_v} = \begin{bmatrix}
            2 \\ -1
        \end{bmatrix} \\
        \Rightarrow & \begin{bmatrix}
            1 & 2 \\ 0 & -1
        \end{bmatrix}
    \end{split} 
\end{equation*}

\subsubsection*{(b)}
\begin{proof}
    By \textbf{Proposition 4.31} and \textbf{Definition of Orthogonal Matrix}, we know that if the columns are not \textbf{Othonomal}, then the matrix must
    not be orthogonal.
    \begin{equation*}
        \begin{split}
            \langle A_1, A_2 \rangle = 2 \neq 0
        \end{split}
    \end{equation*}
    Therefore, $A$ is not orthogonal matrix.
\end{proof}


\subsubsection*{(c)}
\begin{proof}
    Because \textbf{Proposition 4.30} says, \textbf{Suppose} $B_v, B_w$ \textbf{are orthonormal basis of} $\mathbb{V, W}$. But $ \left(\begin{bmatrix}
        1 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        1 \\ 1
    \end{bmatrix} \right) $ itself is not an orthonomal basis.
\end{proof}

\subsection*{4.5.6}
\begin{proof}
    Since $\mathbb{C}_{2 \pi}(\mathbb{R})$ is equipped with innerporduct, so it is both an innerproduct space and a normed space. \\
    Suppose $\forall f(x) \in \mathbb{C}_{2 \pi}(\mathbb{R})$, which means $f(x)$ is a continuous $2 \pi$ periodic function $\Rightarrow$
    $\exists g(x) = f(x - t) \in \mathbb{C}_{2 \pi}(\mathbb{R}), T(g)(x) = f(x + t)$. $T$ is a surjective lineaer map.
    \begin{denote}
        \begin{equation*}
            \begin{split}
                & \int_{a}^{b} f(x) \overline{f(x)} \,d(x) = F(b) - F(a) = \int_{a + 2 \pi}^{b + 2 \pi} f(x) \overline{f(x)} \,d(x) = F(b + 2 \pi ) - F(a+ 2 \pi) \\
                \Rightarrow & F(b + 2 \pi) - F(b) =  F(a + 2 \pi) - F(a)
            \end{split}
        \end{equation*}
    \end{denote}

    \textbf{So we get}
    \begin{equation*}
        \begin{split}
            \|(Tf)(x)\|   \int_{0}^{2 \pi} (Tf)(x) \overline{(Tf)(x)} \,dx & = \int_{0}^{2 \pi} f(x+t) \overline{f(x+t)} \,dx \\
                        & = \int_{0}^{2 \pi} f(x+t) \overline{f(x+t)} \,d(x + t) \\
                        & = \int_{t}^{t + 2 \pi} f(x') \overline{f(x')} \,d(x') \\
                        & = F(t + 2 \pi) - F(t) \\
                        & = F(0 + 2 \pi) - F(0) \\
                        & = \|(f)(x)\|
        \end{split}
    \end{equation*}
    So, it is an isometry.
\end{proof}

\subsection*{4.5.14}
\subsubsection*{(a)}
\begin{proof}
    By \textbf{Proposition 4.30}, we know that if $U$ is unitary, then all columns of $U$ is orthonomal. By \textbf{Theorem 4.3}, we know that all columns are independent. 
    By \textbf{Theorem 3.28}, we know that the columns are orthonomal basis. By \textbf{Corrolary 4.30}, we Know that $U$ is an isometry.
    \begin{equation*}
        \begin{split}
            \|U\|_{op} = \mathop{max} \limits _ {\|v\| = 1, v \in \mathbb{C}^n} \|Uv\| = \|v\| = 1
        \end{split}
    \end{equation*}
\end{proof}

\subsubsection*{(b)}
By \textbf{Proposition 4.30}, we know that if $U$ is unitary, then all columns of $U$ is orthonomal. By \textbf{Theorem 4.3}, we know that all columns are independent. 
By \textbf{Theorem 3.28}, we know that the columns are orthonomal basis. By \textbf{Corrolary 4.30}, we Know that $U$ is an isometry.
\begin{equation*}
    \begin{split}
        \|U\|_F = \sqrt{trU^*U }= \sqrt{trI} = \sqrt{n}
    \end{split}
\end{equation*}

\subsection*{5.1.4}
\begin{denote}
    $e_1 = \begin{bmatrix}
        1 \\ 0 \\ 0
    \end{bmatrix}$
    $e_2 = \begin{bmatrix}
        0 \\ 1 \\ 0
    \end{bmatrix}$
    $e_3 = \begin{bmatrix}
        0 \\ 0 \\ 1
    \end{bmatrix}$
\end{denote}
First use standard basis as $(e_1, e_2, e_3)$. Second use $(e_2, e_3)$ as basis for output. \\
\begin{equation*}
    \begin{split}
        &T(e_1) = \begin{bmatrix}
            0 \\  1 \\ 0
        \end{bmatrix} = 1 * e_2\\
        &T(e_3) = \begin{bmatrix}
            0 \\  0 \\ 1
        \end{bmatrix} = 1 * e_3 \\
        &T(e_2) = \begin{bmatrix}
            0 \\  0 \\ 0
        \end{bmatrix} = 0 * e_2 
    \end{split}
\end{equation*}
Since $rank T = 2$, so the $\sigma_1 = 1, \sigma_2 = 1$ \\
The right vectors are $(e_1, e_3, e_2)$, left vectors are $(e_2, e_3)$, and $\sigma_1 = 1, \sigma_2 = 1, \sigma_3 =0$.

\subsection*{5.1.6}

\begin{denote}
    The standard basis of $\mathbb{V}$ is $(e_1, e_2, ..., e_n)$, with $dimV = n$. As well as $P := P_U$ with $U \subset V, dim U = m, U = span(e_1, ..., e_m)$, and $V = U \oplus  U^\bot$.
\end{denote}
\subsubsection*{(a)}

By \textbf{Theorem 5.3}, we know that the singular value are unique. So we just need to prove that we can find only $1, 0$. \\
By \textbf{Theorem 4.16.2}, when $i \le m$
\begin{equation*}
    \begin{split}
        P_U(e_i) = \sum_{j = 1}^{j = m} \langle e_i, e_j \rangle e_j = \langle e_i, e_{j=i} \rangle e_{j = i} = 1 * e_i
    \end{split}
\end{equation*}
By \textbf{Theorem 4.16.2}, when $i > m$
\begin{equation*}
    \begin{split}
        P_U(e_i) = \sum_{j = 1}^{j = m} \langle e_i, e_j \rangle e_j = 0
    \end{split}
\end{equation*}
So, the singular values are only $0, 1$. 

\subsubsection*{(b)}
I choose $(e_1, e_2, ..., e_n)$ as both left and right singular vectors, and with the same sequence.

\subsection*{5.1.10}
\begin{proof}
    Since $T$ is invertible, it must be surjective, so $n = rangeT = dimW = dimV$. Since $\forall i \in \{1, 2, ..., n\}, \sigma_i > 0, \ne 0$. By \textbf{Theorem5.3}, we know the singular value is unique.
    
    \begin{denote}
        The orthonomal basis of $V$ is $(e_1, e_2, ..., e_n)$, and the orthonomal basis of $W$ is $(f_1, f_2, ..., f_n)$.
    \end{denote}
    \begin{consider}
        Construct $T^{-1} \in \mathfrak{L}(W, V)$, with singular value $\sigma'_1, \sigma'_2, ..., \sigma'_n$
    \end{consider}
    Since $\forall i \in \{1, 2, ..., n\}$
    \begin{equation*}
        \begin{split}
            T(e_i) = \sigma_if_i \Rightarrow T^{-1}(f_i) = \frac{1}{\sigma_i}e_i
        \end{split}
    \end{equation*}
    $\forall i \ge j$
    \begin{equation*}
        \begin{split}
            \sigma_i \ge \sigma_j \Rightarrow \frac{1}{\sigma_j} \ge \frac{1}{\sigma_i}
        \end{split}
    \end{equation*}
    Therefore, $\{ \frac{1}{\sigma_n}, \frac{1}{\sigma_{n-1}}, ..., \frac{1}{\sigma_1} \}$ are singular value of $T^{-1}$. \\
    By \textbf{{Key Ideas On Page295 last point}}, we know that the largest singular value is operator norm of that map.
    \begin{equation*}
        \begin{split}
            \|T^{-1}\|_{op} = \frac{1}{\sigma_n} \Rightarrow \|T^{-1}\|^{-1}_{op} = \sigma_n
        \end{split}
    \end{equation*}
    \begin{denote}
        $v = \sum_{i = 1}^{i = n} a_ie_i $, $\|v\| = 1$
    \end{denote}
    \begin{equation*}
        \begin{split}
            \|T(v)\| & = \|T(\sum_{i = 1}^{i = n} a_ie_i)\| = \|\sum_{i = 1}^{i = n} a_iT(e_i)\| = \|\sum_{i = 1}^{i = n} a_i\sigma_if_i\| \\
                & = \sqrt{\sum_{i = 1}^{i = n} |\sigma_i|^2\|a_if_i\|^2} \\
                & \ge |\sigma_n|\sqrt{\sum_{i = 1}^{i = n} |a_i|^2\|f_i\|^2} \\
                & = |\sigma_n|\sqrt{\sum_{i = 1}^{i = n} |a_i|^2} = |\sigma_n| \\
                & \Rightarrow \mathop{min} \limits _ {\|v\| = 1}\|Tv\| = \sigma_n
        \end{split}
    \end{equation*}

\end{proof}

\subsection*{5.1.14}
\subsubsection*{(a)}
\begin{denote}
    The orthonomal basis of $\mathbb{W}$ is $(f_1, f_2, ..., f_m)$, with $dim \mathbb{W} = m$
\end{denote}
\begin{equation*}
    \begin{split}
        \|Tv\| & = \|T(\sum_{i = j}^{i=p} a_ie_i + \sum_{i = p+1}^{i=n} a_ie_i)\| = \| \sum_{i = j}^{i=p} a_iT(e_i) + \sum_{i = p+1}^{i=n} a_iT(e_i)\| \\
            & = \| \sum_{i = j}^{i=p} a_i\sigma_if_i + \sum_{i = p+1}^{i=n} 0 \| = \| \sum_{i = j}^{i=p} a_i\sigma_if_i\| \\
            & = \sqrt{\sum_{i = j}^{i=p} \|a_i\sigma_if_i\|^2} \le \sqrt{|\sigma_j|^2\sum_{i = j}^{i=p} \|a_if_i\|^2} \\
            & \le \sqrt{|\sigma_j|^2\sum_{i = j}^{i=n} \|a_if_i\|^2} = \sqrt{|\sigma_j|^2\sum_{i = j}^{i=n} \|a_i\|^2} \\ & = \sigma_j\|v\|
    \end{split} 
\end{equation*}

\subsubsection*{(b)}
\begin{denote}
    The orthonomal basis of $\mathbb{W}$ is $(f_1, f_2, ..., f_m)$, with $dim \mathbb{W} = m$
\end{denote}
\begin{equation*}
    \begin{split}
        \|Tv\| &= \|T(\sum_{i = 1}^{i=j} a_ie_i)\|  =  \|\sum_{i = 1}^{i=j} a_iT(e_i)\|\\
        & = \|\sum_{i = 1}^{i=j} a_i \sigma_i f_i\| = \sqrt{\sum_{i = 1}^{i=j}\| a_i \sigma_i f_i\|^2} \\
        & \ge  \sqrt{|\sigma_j|^2\sum_{i = 1}^{i=j}\| a_i  f_i\|^2} = \sqrt{|\sigma_j|^2\sum_{i = 1}^{i=j}\| a_i \|^2} \\
        & = \sigma_j \| v\|
    \end{split} 
\end{equation*}

\subsubsection*{(c)}
First I could build a vector space $V = \langle e_1, e_2, ..., e_j\rangle, dimV = j$. By \textbf{Lemma 3.22}, \begin{equation*}
    \begin{split}
        dimU + dimV = n - j + 1 + j \ge n \Rightarrow U \cap V \ne {0}
    \end{split}
\end{equation*}
By conclusion from \textbf{Part (b)}
\begin{equation*}
    \begin{split}
        \exists v_0 \in U, v_0 \in V, \|Tv_0\| \ge \sigma_j v_0
    \end{split}
\end{equation*}

\subsubsection*{(d)}
Use the conclusion from \textbf{Part (c)}, we know that, if $dimU = n - j + 1 \Rightarrow \exists v_0 \in U, \|Tv\| \ge \sigma_j\|v_0\|$
 This is equivalent to \begin{equation*}
     \begin{split}
         dimU = n - j + 1 \Rightarrow \mathop{max} \limits _{v \in U} \|Tv\| \ge \sigma_j\|v\|
     \end{split}
 \end{equation*}

 When $\forall v \in U, \|v\| = 1$, we get
 \begin{equation*}
    \begin{split}
        dimU = n - j + 1 \Rightarrow \mathop{max} \limits _{v \in U, \|v\| = 1} \|Tv\| \ge \sigma_j
    \end{split}
\end{equation*}

Since it is true $\forall U \subset V, dimU = n - j + 1$, the statement is equivalent to \begin{equation}
    \begin{split}
        \mathop{min} \limits_{dimU = n - j + 1} \mathop{max} \limits _{v \in U, \|v\| = 1} \|Tv\| \ge \sigma_j
    \end{split}
\end{equation}

Now we need to prove another side. Now we build $U' = \langle e_j, e_{j+1}, ..., e_n \rangle, dimU' = n - j + 1$. From conclusion from \textbf{Part (a)}, we get
\begin{equation}
    \begin{split}
        \mathop{max} \limits _{v \in U', \|v\| = 1} \|Tv\| \le \sigma_j
    \end{split}
\end{equation}

From \textbf{Equation (1), and (2)}, we get 
\begin{equation}
    \begin{split}
        \mathop{min} \limits_{dimU = n - j + 1} \mathop{max} \limits _{v \in U, \|v\| = 1} \|Tv\| \le \mathop{max} \limits _{v \in U', \|v\| = 1} \|Tv\| \le \sigma_j
    \end{split}
\end{equation}

From \textbf{Eqution (1), and (3)}, I prove \begin{equation*}
    \begin{split}
        \mathop{min} \limits_{dimU = n - j + 1} \mathop{max} \limits _{v \in U, \|v\| = 1} \|Tv\| = \sigma_j
    \end{split}
\end{equation*} 

\subsection*{5.2.2}
\subsubsection*{(a)}
The singualr value of matrix $A$ is the sqrt root of eignvealue of  $AA^*$ or $AA^*$.
\begin{equation*}
    \begin{split}
        \begin{bmatrix}
            1 & -2 \\ 2 & 1
        \end{bmatrix} \begin{bmatrix}
            1 & 2 \\ -2 & 1
        \end{bmatrix} = \begin{bmatrix}
            5 & 0 \\ 0 & 5
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is an uppertraingular matrix, so the eignvalue is its diagonal, which is $5$. And it gets two independent eignvectors\\
So the singualr value is $\sigma_1 = \sigma_2 = \sqrt{5}$.

\subsubsection*{(b)}
The singualr value of matrix $A$ is the sqrt root of eignvealue of  $AA^*$ or $AA^*$.
\begin{equation*}
    \begin{split}
        \begin{bmatrix}
            4 & -3 \\ 6 & 8
        \end{bmatrix} \begin{bmatrix}
            4 & 6 \\ -3 & 8
        \end{bmatrix} = \begin{bmatrix}
            25 & 0 \\ 0 & 100
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is an uppertraingular matrix, so the eignvalue is its diagonal, which is $25, 100$. And it gets two independent eignvectors\\
So the singualr value is $\sigma_1 = 10, \sigma_2 = 5$.

\subsubsection*{(c)}
The singualr value of matrix $A$ is the sqrt root of eignvealue of  $AA^*$ or $AA^*$.
\begin{equation*}
    \begin{split}
        \begin{bmatrix}
            i & 0 & 0 \\ 0 & \sqrt{2} & 0
        \end{bmatrix} \begin{bmatrix}
            -i & 0 \\ 0 & \sqrt{2} \\ 0 & 0
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 \\ 0 & 2
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is an uppertraingular matrix, so the eignvalue is its diagonal, which is $2, 1$.\\
So the singualr value is $\sigma_1 = \sqrt{2}, \sigma_2 = 1$.

\subsubsection*{(d)}
The singualr value of matrix $A$ is the sqrt root of eignvealue of $AA^*$ or $AA^*$.
\begin{equation*}
    \begin{split}
        \begin{bmatrix}
            1 & 2 & 1 \\ 1 & -1 & 1
        \end{bmatrix}\begin{bmatrix}
            1 & 1 \\ 2 & -1 \\ 1 & 1
        \end{bmatrix} = \begin{bmatrix}
            6 & 0 \\ 0 & 3 
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is an uppertraingular matrix, so the eignvalue is its diagonal, which is $6, 3$.\\
So the singualr value is $\sigma_1 = \sqrt{6}, \sigma_2 = \sqrt{3}$.

\subsubsection*{(e)}
The singualr value of matrix $A$ is the sqrt root of eignvealue of $AA^*$ or $AA^*$.
\begin{equation*}
    \begin{split}
        \begin{bmatrix}
            0 & 1 & 0 \\ 0 & 0 &2 \\ 3 & 0 & 0
        \end{bmatrix} \begin{bmatrix}
            0 & 0 & 3 \\ 1 & 0 & 0 \\ 0 & 2 & 0
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 9
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is an uppertraingular matrix, so the eignvalue is its diagonal, which is$9, 4, 1$ \\
So the singualr value is $\sigma_1 = 3, \sigma_2 = 2, \sigma_3 = 1$.

\subsection*{5.2.8}
By \textbf{Proposition 5.5}, we know the singular valeu of $A$ is positive sqrt root of eignvalue of $A^*A$ or $AA^*$.
\begin{equation*}
    \begin{split}
        A^*A = \begin{bmatrix}
            \lambda_1 & 0 & 0 & ... & 0 \\
            0 & \lambda_2 & 0 & ... & 0 \\
            0 & 0 & \lambda_3 & ... & 0 \\
            ...\\
            0 & 0 & 0 & ... & \lambda_n 
        \end{bmatrix} \begin{bmatrix}
            \overline{\lambda_1} & 0 & 0 & ... & 0 \\
            0 & \overline{\lambda_2} & 0 & ... & 0 \\
            0 & 0 & \overline{\lambda_3} & ... & 0 \\
            ...\\
            0 & 0 & 0 & ... & \overline{\lambda_n}
        \end{bmatrix} = \begin{bmatrix}
            |\lambda_1|^2 & 0 & 0 & ... & 0 \\
            0 & |\lambda_2|^2 & 0 & ... & 0 \\
            0 & 0 & |\lambda_3|^2 & ... & 0 \\
            ...\\
            0 & 0 & 0 & ... & |\lambda_n|^2
        \end{bmatrix}
    \end{split}
\end{equation*}
Since it is a diagonal matrix, so the eignvalue of $A^*A$ is its diagonal. Singualr value of $A$ is sqrt root of its eignvalue $\Rightarrow$
The singular value of $A$ is $|\lambda_1|, |\lambda_2|, |\lambda_3|, ..., |\lambda_n|$


\subsection*{5.2.16}
\subsubsection*{(a)}
\begin{equation*}
    \begin{split}
        AA^\dagger b = b \Rightarrow \exists x, x= A^\dagger b, Ax = b
    \end{split}
\end{equation*}
From the conclusion \textbf{5.2.15(d)}
\begin{equation*}
    \begin{split}
        \exists x, Ax = b \Rightarrow AA^\dagger A x = Ax = b \Rightarrow AA^\dagger b = b
    \end{split}
\end{equation*}
\subsubsection*{(b)}
First showo that: 
\begin{equation*}
    \begin{split}
        \Sigma ^ \dagger \Sigma = diag(1, ..., 1, 0, .., 0)
    \end{split}
\end{equation*}
Since the system is consistent
\begin{equation*}
    \begin{split}
        A^\dagger Ax = A^\dagger b 
    \end{split}
\end{equation*}
So we get, $U, V, U^*, V^* $ are isometry.
\begin{equation*}
    \begin{split}
        \|A^\dagger b \| &= \| A^\dagger Ax \| = \|V \Sigma ^ \dagger U^* U \Sigma V^* x\| = \| V \Sigma ^ \dagger \Sigma V^* x\| \\
                        & = \| \Sigma ^ \dagger \Sigma V^*x \| \le \|I V^*x\| = \|V^*x\| = \|x\|
    \end{split}
\end{equation*}

\subsubsection*{(c)}
\begin{equation*}
    \begin{split}
        \forall v_0 \in kerA, Av_0 = 0 \Rightarrow (I_n - A^\dagger A)v_0 = v_0 - 0 = v_0 \Rightarrow v_0 \in C(I - A^\dagger A)
    \end{split}
\end{equation*}
So we get 
\begin{equation*}
    \begin{split}
        kerA \subset C(I - A^\dagger A)
    \end{split}
\end{equation*}
For another side
\begin{equation*}
    \begin{split}
       &\forall x \in C(I - A^\dagger A), x = (I - A^\dagger A)v_0 \Rightarrow Ax = A (I - A^\dagger A)v_0 = (A -  AA^\dagger A)v_0 = 0 \\
        \Rightarrow & C(I - A^\dagger A) \subset kerA 
    \end{split}
\end{equation*}

We get 
\begin{equation*}
    \begin{split}
        C(I - A^\dagger A) = kerA 
    \end{split}
\end{equation*}

Since $Ax = b$ is consistent, so $AA^\dagger b = b$, $A ^\dagger b$ is one of the solution. By \textbf{Proposition 2.42}
\begin{equation*}
    \begin{split}
        \{ x \in F^n | Ax = b\} = \{ A^\dagger b + k | k \in KerA \} = \{ A^\dagger b + [I_n - A^\dagger A]w | w \in F^n \}
    \end{split}
\end{equation*}

\subsection*{5.2.20}
At each $ith$ position:
\begin{equation*}
    \begin{split}
        \Sigma = \sum_{i = 1}^{i = r} diag(0, 0, ..., \sigma_i, 0..., 0)
    \end{split}
\end{equation*}
So we get 
\begin{equation*}
    \begin{split}
        A & = U \Sigma V^* = U (\sum_{i = 1}^{i = r} diag(0, 0, ..., \sigma_i, 0..., 0))  V^* \\
        & = U\sum_{i = 1}^{i = r} diag(0, 0, ..., \sigma_i, 0..., 0)) V^* \\
        & = U(\sum_{i = 1}^{i = r} \begin{bmatrix}
            0^* \\ 0^* \\ ... \\ \sigma_i(V_i)^* \\ 0^* \\ ... \\ 0^*
        \end{bmatrix}) = \sum_{i = 1}^{i = r} U \begin{bmatrix}
            0^* \\ 0^* \\ ... \\ \sigma_i(V_i)^* \\ 0^* \\ ... \\ 0^*
        \end{bmatrix} \\
        & = \sum_{i = 1}^{i = r} u_i \sigma_i (v_i)^* = \sum_{i = 1}^{i = r} \sigma_i u_i  (v_i)^*
    \end{split}
\end{equation*}




\end{document}