\documentclass{article}
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{ntheorem}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem*{proposition}{Proposition}
\newtheorem*{definition}{Definition}
\newtheorem*{corrolary}{Corrolary}
\newtheorem*{consider}{Consider}
\newtheorem*{theorem}{Theorem}
\newtheorem*{suppose}{Suppose}
\newtheorem*{notice}{Notice}
\newtheorem*{define}{Define}
\newtheorem*{denote}{Denote}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}
\newtheorem*{proof}{Proof}
\newtheorem*{case}{Case}
%\begin{equation*} \end{equation*}%
%\begin{equation} \end{equation}%
%\begin{split} \end{split}%
%\begin{cases} \end{cases}%
%\subsubsection*{(a)}%
%\subsection*{(a)}%
%\textbf{}%
%\textit{}%
%\noindent%
%\paragraph{}%
%\page{}%
%\\%
%$\langle \rangle$%
%$\| \|$%
% \begin{equation*}
%     \begin{split}
%         & (\lfloor \frac{t}{d_3} \rfloor - 2p) + f((t \mod d_3) + 2 d_3 p, 2) \\
%         \rightarrow & (\lfloor \frac{t}{d_3} \rfloor - 2p) 
%         + \lfloor \frac{(t \mod d_3) + 2 d_3 p}{d_2} \rfloor 
%         + f(((t \mod d_3) + 2 d_3 p) \mod d_2, 1) \\
%         = & (\lfloor \frac{t}{25} \rfloor - 2p) 
%         + \lfloor \frac{(t \mod 25) + 2 \cdot 25 \cdot p}{10} \rfloor 
%         + f(((t \mod 25) + 2 \cdot 25 \cdot p) \mod 10, 1) \\
%         = & (\lfloor \frac{t}{25} \rfloor - 2p) 
%         + \lfloor \frac{t \mod 25}{10} \rfloor + 5 p 
%         + f((t \mod 25) \mod 10, 1) \\
%         = & \lfloor \frac{t}{25} \rfloor
%         + \lfloor \frac{t \mod 25}{10} \rfloor + 3 p 
%         + f((t \mod 25) \mod 10, 1) 
%     \end{split}
%     \end{equation*}

% \begin{equation}
%     \begin{split}
%     (a + b)^4
%       &= (a + b)^2 (a + b)^2     \\
%       &= (a^2 + 2ab + b^2)
%         (a^2 + 2ab + b^2)       \\
%       &= a^4 + 4a^3b + 6a^2b^2 + 4ab^3 + b^4
%     \end{split}
%    \end{equation}

\geometry{a4paper}
\title{Homework10}
\author{Zhihao Wang} 
\date{04/08/2022}
\begin{document}
\maketitle 

\subsection*{4.1.4}
\begin{denote}
    The $jth$ col of $A, B$ is $A_j, B_j$. And $A_j, B_j \in \mathbb{C}^m$. So that, $\|A_j\|^2 = \langle A_j, A_j\rangle$, as well as for $B_j$.
\end{denote}
\begin{proof}
    By the definition of inner product $\sum_{i=1}^{i=m} [A]_{ij}[B]_{ij} =\langle A_j, B_j \rangle$.
    For the inner product of two matrices: $\langle A, B\rangle _{F} =\sum_{j=1}^{j=n} \sum_{i=1}^{i=m} [A]_{ij}[B]_{ij} =\sum_{j=1}^{j=n} \langle A_j, B_j \rangle$. 
    By the definition of the norm: $\|A\|^2 {_F}  = \langle A, A \rangle _F = \sum_{j=1}^{j=n} \langle A_j, A_j \rangle = \sum_{j=1}^{j=n}\|A_j\|^2$.
\end{proof}

\subsection*{4.1.8}
\noindent Suppose there exists $w \in V$. $\langle v - w, v - w \rangle = \langle w - v, w - v \rangle \Rightarrow \|v - w\| = \|w - v\|$
\begin{equation*}
    \begin{split}
          \|v\| = \|v - u + u\| \le \|v - u\| + \|u\| \Rightarrow &\|v - u\| \ge \|v\| - \|u\| = 11 - 2 = 9 \\
        & \|v - u\| \le \|v-w\| + \|w - u\| = \|v-w\| + \|u - w\| = 8
    \end{split}
\end{equation*}
Contradiction.

\subsection*{4.1.14}
\subsubsection*{(a)}
\begin{equation*}
    \begin{split}
        \frac 1 4 (\|v + w\|^2 - \|v - w\|^2) = \frac 1 4 (\langle v+w, v+w\rangle - \langle v-w, v-w \rangle) = \frac 1 4(2 \langle v, w \rangle - (-2) \langle v, w \rangle) = \langle v, w \rangle
    \end{split}
\end{equation*}

\subsubsection*{(b)}
\begin{equation*}
    \begin{split}
        &\frac 1 4 (\|v + w\|^2 - \|v - w\|^2 + i\|v + iw\|^2 - i\|v-iw\|^2) \\
        &= \frac 1 4 (\langle v+w, v+w\rangle - \langle v-w, v-w \rangle + i(\langle v+ iw, v+ iw\rangle - \langle v- iw, v- iw \rangle)) \\
        &= \frac 1 4(2 \langle w , v\rangle +2 \langle v, w \rangle + i(2i \langle w , v\rangle - 2i \langle v, w \rangle)) = \langle v, w \rangle
    \end{split}
\end{equation*}

\subsection*{4.1.20}
\begin{proof}
    $\forall v, w \in \mathbb{V}$. Use \textbf{Definition} and \textbf{Proposition 4.2.2}.
    \begin{equation*}
        \begin{split}
            & \langle 0, v \rangle = 0 * \langle w, v \rangle = 0 \\
            & \langle v, 0 \rangle = \overline 0 * \langle v, w \rangle = 0
        \end{split}
    \end{equation*}
\end{proof}

\subsection*{4.2.4}
\noindent This Exercise we need to ouse \textbf{Theorem 4.9} 

\subsubsection*{(a)}
\begin{equation*}
    \begin{split}
        & \langle \begin{bmatrix}
            1 \\ 2 \\ -3
        \end{bmatrix}, \frac{1}{ \sqrt{2}} \begin{bmatrix}
            1 \\ 0 \\ -1
        \end{bmatrix} \rangle = 2 \sqrt{2} \\
        & \langle \begin{bmatrix}
            1 \\ 2 \\ -3
        \end{bmatrix}, \frac{1}{ \sqrt{6}} \begin{bmatrix}
            1 \\ -2 \\ 1
        \end{bmatrix} \rangle = - \sqrt{6}
    \end{split}
    \Rightarrow \begin{bmatrix}
        2 \sqrt{2} \\ - \sqrt{6}
    \end{bmatrix}
\end{equation*}

\subsubsection*{(b)}
Refer to \textbf{Exercise 4.1.4}
\begin{equation*}
    \begin{split}
        & \langle \begin{bmatrix}
            5 & 7 \\7 & 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 & 1 \\ 1 & 1
        \end{bmatrix} \rangle = \frac{21}{2} \\
        & \langle \begin{bmatrix}
            5 & 7 \\7 & 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 & 1 \\ -1 & -1
        \end{bmatrix} \rangle = \frac{3}{2} \\
        & \langle \begin{bmatrix}
            5 & 7 \\7 & 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 & -1 \\ 1 & -1
        \end{bmatrix} \rangle = \frac{3}{2} \\
        & \langle \begin{bmatrix}
            5 & 7 \\7 & 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 & -1 \\ -1 & 1
        \end{bmatrix} \rangle = \frac{-7}{2} 
    \end{split}
    \Rightarrow \begin{bmatrix}
        \frac{21}{2} \\ \frac{3}{2} \\ \frac{3}{2} \\ \frac{-7}{2} 
    \end{bmatrix}
\end{equation*}

\subsubsection*{(c)}
\begin{equation*}
    \begin{split}
        & \langle
        \begin{bmatrix}
            -3 \\ 0 \\ 1 \\ 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 \\ 1 \\ 1 \\ 1
        \end{bmatrix}
        \rangle = 0 \\
        & \langle
        \begin{bmatrix}
            -3 \\ 0 \\ 1 \\ 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 \\ i \\ -1 \\ -i
        \end{bmatrix}
        \rangle = -2 + i \\
        & \langle
        \begin{bmatrix}
            -3 \\ 0 \\ 1 \\ 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 \\ -1 \\ 1 \\ -1
        \end{bmatrix}
        \rangle = -2 \\
        & \langle
        \begin{bmatrix}
            -3 \\ 0 \\ 1 \\ 2
        \end{bmatrix}, \frac{1}{2} \begin{bmatrix}
            1 \\ -i \\ -1 \\ i
        \end{bmatrix}
        \rangle = -2 - i
    \end{split}
    \Rightarrow \begin{bmatrix}
        0 \\ -2 + i \\ -2 \\ -2 - i
    \end{bmatrix}
\end{equation*}

\subsection*{4.2.6}
\noindent This question we will use \textbf{Theorem 4.9}. Assume the $ith$ row and $jth$ column of the matrix is the following equation: 
\begin{equation*}
    \begin{split}
        [[T]_{B_{\mathbb{V}}B_{\mathbb{V}}}]_{ij} = \langle T(e_j), e_i \rangle
    \end{split}
\end{equation*}

\subsubsection*{(a)}
\begin{equation*}
    \begin{split}
        &[[T]_{B_\mathbb{V}}]_{11} = \langle T(e_1), e_1 \rangle = \langle \frac{1}{\sqrt{2}} \begin{bmatrix}
            0 \\ -1 \\ 1
        \end{bmatrix}, \frac{1}{\sqrt{2}}\begin{bmatrix}
            1 \\ 0 \\ -1
        \end{bmatrix} \rangle = - \frac{1}{2}\\
        &[[T]_{B_\mathbb{V}}]_{21} = \langle T(e_1), e_2 \rangle =  \langle \frac{1}{\sqrt{2}} \begin{bmatrix}
            0 \\ -1 \\ 1
        \end{bmatrix}, \frac{1}{\sqrt{6}}\begin{bmatrix}
            1 \\ -2 \\ 1
        \end{bmatrix} \rangle = \frac{\sqrt{3}}{2}\\
        &[[T]_{B_\mathbb{V}}]_{12} = \langle T(e_2), e_1 \rangle =  \langle \frac{1}{\sqrt{6}} \begin{bmatrix}
            -2 \\ 1 \\ 1
        \end{bmatrix}, \frac{1}{\sqrt{2}}\begin{bmatrix}
            1 \\ 0 \\ -1
        \end{bmatrix} \rangle = - \frac{\sqrt{3}}{2}\\
        &[[T]_{B_\mathbb{V}}]_{22} = \langle T(e_2), e_2 \rangle =  \langle \frac{1}{\sqrt{6}} \begin{bmatrix}
            -2 \\ 1 \\ 1
        \end{bmatrix}, \frac{1}{\sqrt{6}}\begin{bmatrix}
            1 \\ -2 \\ 1
        \end{bmatrix} \rangle = - \frac{1}{2}
    \end{split}
    \Rightarrow \begin{bmatrix}
        - \frac{1}{2} & - \frac{\sqrt{3}}{2}\\ \frac{\sqrt{3}}{2} & - \frac{1}{2}
    \end{bmatrix}
\end{equation*}

\subsubsection*{(b)}
\begin{equation*}
    \begin{split}
        &
        \begin{bmatrix}
            \langle T(e_1), e_1\rangle & \langle T(e_2), e_1\rangle & \langle T(e_3), e_1\rangle & \langle T(e_4), e_1\rangle\\ 
            \langle T(e_1), e_2\rangle & \langle T(e_2), e_2\rangle & \langle T(e_3), e_2\rangle & \langle T(e_4), e_2\rangle\\ 
            \langle T(e_1), e_3\rangle & \langle T(e_2), e_3\rangle & \langle T(e_3), e_3\rangle & \langle T(e_4), e_3\rangle\\ 
            \langle T(e_1), e_4\rangle & \langle T(e_2), e_4\rangle & \langle T(e_3), e_4\rangle & \langle T(e_4), e_4\rangle\\ 
        \end{bmatrix} \\
        & = 
        \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
        \end{bmatrix}
    \end{split}
\end{equation*}

\subsubsection*{(c)}
\begin{equation*}
    \begin{split}
        &
        \begin{bmatrix}
            \langle T(e_1), e_1\rangle & \langle T(e_2), e_1\rangle & \langle T(e_3), e_1\rangle & \langle T(e_4), e_1\rangle\\ 
            \langle T(e_1), e_2\rangle & \langle T(e_2), e_2\rangle & \langle T(e_3), e_2\rangle & \langle T(e_4), e_2\rangle\\ 
            \langle T(e_1), e_3\rangle & \langle T(e_2), e_3\rangle & \langle T(e_3), e_3\rangle & \langle T(e_4), e_3\rangle\\ 
            \langle T(e_1), e_4\rangle & \langle T(e_2), e_4\rangle & \langle T(e_3), e_4\rangle & \langle T(e_4), e_4\rangle\\
        \end{bmatrix} \\
        & =
        \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 0 & 0 & -i \\ 
            0 & 0 & -1 & 0 \\
            0 & i & 0 & 0 
        \end{bmatrix}
    \end{split}
\end{equation*}

\subsection*{4.2.8}
\noindent The main idea is use orthonormal basis to help use calculate the innerproduct.
\begin{proof}
    Because we are using $\mathbb{R}$ for this question, so $|\langle v, w\rangle|^2 = \langle v, w\rangle * \overline {\langle v, w\rangle} = \langle v, w\rangle^2$. \\
    Use the \textbf{Theorem 4.10}, and according to the example on \textbf{Page 240}:
    \begin{denote}
        The othonomal basis of triangonometric polynomial space is $ B_v = \{ \frac {1} {\sqrt {2 \pi}}, \frac{1}{\sqrt{\pi}} sin(kx), \frac{1}{\sqrt{\pi}} cos(lx) | k = 1, ... n \in \mathbb{N}, l = 1, ... m \in \mathbb{N} \}$. 
        The $ith$ entry of $[f]_{B_v}$, which is under the triangonometric polynomial space is $t_i = \langle f, e_i\rangle$. 
    \end{denote}
    Since the element of the function is constitute by the orthonoml basis. So when any part of the function, $b_0, a_ksin(k\theta), b_lcos(l\theta)$, we can tret them as constant times of one of the orthonomal basis.
    \begin{equation*}
        \begin{split}
            \|f\|^ 2 & = \sum_{i=1}^{i=1+m+n} | \langle f, e_i\rangle|^2 = \sum_{i=1}^{i=1+m+n}  \langle f, e_i\rangle ^2 = \sum_{i=1}^{i=1+m+n} t_i ^2 \\
            & = \sum_{i=1}^{i=1+m+n} (| \langle \sum_{k=1}^{k=n}a_ksin(k \theta) , e_i\rangle |^2 + | \langle \sum_{l=1}^{l=m}b_lcos(l \theta) , e_i\rangle |^2  + | \langle b_0, e_i \rangle|^2)\\
            & = \sum_{i=1}^{i=1+m+n} (| \langle \sqrt{\pi} \sum_{k=1}^{k=n} \frac{1}{\sqrt{\pi}}a_ksin(k \theta) , e_i\rangle |^2 + | \langle \sqrt{\pi} \sum_{l=1}^{l=m} \frac{1}{\sqrt{\pi}}b_lcos(l \theta) , e_i\rangle |^2  + | \sqrt{2\pi} b_0 \langle \frac{1}{\sqrt{2\pi}}, e_i \rangle|^2)\\
            & = {b_0 ^ 2}{2 \pi} + \sum_{k=1}^{k=n} {a_k ^2 }{\pi} + \sum_{l=1}^{l=m} {b_l ^2 }{\pi} \\
            & \Rightarrow \|f\| = \sqrt{{b_0^2}{{2 \pi}} + \sum_{k=1}^{k=n}  {a_k ^2 }{\pi} + \sum_{l=1}^{l=m}{b_l ^2 }{\pi}}
        \end{split}
    \end{equation*}
\end{proof}

\subsection*{4.2.10}
Because we are using $\mathbb{R}$ for this question, so $\forall k \in \mathbb{F}, |k|^2 = k * \overline k = k^2$.
\subsubsection*{(a)}
\begin{proof}
    \begin{equation*}
        \begin{split}
            \|3 - 2x + x^2 \|^2 & = \langle 3 - 2x + x^2, 3 - 2x + x^2 \rangle = \int_{0}^{1} (3 - 2x + x^2) \overline {3 - 2x + x^2} \,dx \\
                                & = \int_{0}^{1} (3 - 2x + x^2)^2 \,dx = [\frac{(x - 1)^5}{5} + \frac{4(x - 1)^3}{3} + 4x - 4 + C ]^1_0 = \frac{83}{15}
        \end{split}
    \end{equation*} 
\end{proof}

\subsubsection*{(b)}
\begin{proof}
    By refering to the conclusion on \textbf{Page 240.6}, we know that $\{1, x, x^2\}$ is not even orthogonal. Therefore, they can not bt othonomal basis. So it will not follow tht \textbf{Theorem 4.10}
\end{proof}

\subsection*{4.2.18}
\subsubsection*{(a)}
\begin{denote}
    $A^*$ is the conjugate transpose of $A$. $[A]_{ij}= \overline {[A^*]_{ji}}$.
\end{denote}
\begin{proof}
    Since the basis we are using is orthonomal basis.  \\ By \textbf{Theorem 4.10}:
\begin{equation*}
    \begin{split}
        \langle x, y\rangle_A = \langle Ax, Ay\rangle \Rightarrow \langle e_j, e_k\rangle_A = \langle Ae_j, Ae_k\rangle = \sum_{i=1}^{i=n} 
    \langle Ae_j, e_i \rangle \overline {\langle Ae_k, e_i \rangle} = \sum_{i=1}^{i=n} [A]_{ij}[A^*]_{ki} = [A^*A]_{kj}
    \end{split}
\end{equation*}
\end{proof}

\subsubsection*{(b)}
\begin{proof}
    Use the formula we get from \textbf{(a)}, and refer to the \textbf{Definition Of Orthonormal Basis}: 
    \begin{equation*}
        \begin{split}
            & k \neq j \rightarrow 0 = \langle e_j, e_k\rangle_A = [A^*A]_{kj} = 0 = [I]_{kj} = [A^{-1}A]_{kj}\\
            & k = j \rightarrow 1 = \langle e_j, e_j\rangle_A = [A^*A]_{jj} = 1 = [I]_{jj} = [A^{-1}A]_{jj}
        \end{split}
        \Rightarrow A^* = A^{-1}
    \end{equation*}
\end{proof}

\subsection*{4.3.2}
\subsubsection*{(a)}
Refer to \textbf{Proposition 4.18}
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            1 & 1 \\
            1 & 2 \\
            1 & 3 \\
            1 & 4
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            1 & 1 \\
            1 & 2 \\
            1 & 3 \\
            1 & 4
        \end{bmatrix} (\begin{bmatrix}
            1 & 1 & 1 & 1 \\
            1 & 2 & 3 & 4
        \end{bmatrix} \begin{bmatrix}
            1 & 1 \\
            1 & 2 \\
            1 & 3 \\
            1 & 4
        \end{bmatrix})^{-1} \begin{bmatrix}
            1 & 1 & 1 & 1 \\
            1 & 2 & 3 & 4
        \end{bmatrix} = \frac{1}{10}\begin{bmatrix}
            7 & 4 & 1 & -2 \\
            4 & 3 & 2 & 1 \\
            1 & 2 & 3 &4 \\
            -2 & 1 & 4 & 7
        \end{bmatrix}
    \end{split}
\end{equation*}

\subsubsection*{(b)}
Refer to \textbf{Proposition 4.18}
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            1 & 0 \\
            i & 1 \\
            0 & i 
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            1 & 0 \\
            i & 1 \\
            0 & i 
        \end{bmatrix} (\begin{bmatrix}
            1 & -i & 0  \\
            0 & 1 & -i 
        \end{bmatrix} \begin{bmatrix}
            1 & 0 \\
            i & 1 \\
            0 & i 
        \end{bmatrix})^{-1} \begin{bmatrix}
            1 & -i & 0  \\
            0 & 1 & -i 
        \end{bmatrix} = \frac{1}{3}\begin{bmatrix}
            2 & -i & 1  \\
            i & 2 & -i  \\
            1 & i & 2 
        \end{bmatrix}
    \end{split}
\end{equation*}

\subsection*{4.3.4}
\subsubsection*{(a)}
Refer to the \textbf{Theorem 4.16.8}
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            3 \\
            1 \\
            4 
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            3 \\
            1 \\
            4  
        \end{bmatrix} (\begin{bmatrix}
            3 & 1 & 4 
        \end{bmatrix} \begin{bmatrix}
            3 \\
            1 \\
            4 
        \end{bmatrix})^{-1} \begin{bmatrix}
            3 & 1 & 4 
        \end{bmatrix} = \frac{1}{26}\begin{bmatrix}
                9 & 3 & 12 \\
                3 & 1 & 4 \\
                12 & 4 & 16
        \end{bmatrix} \\
        & [P_{U^{\bot}}]_\xi = I - [P_U]_\xi = \frac{1}{26}(\begin{bmatrix}
            26 & 0 & 0 \\
            0 & 26 & 0 \\
            0 & 0 & 26 
    \end{bmatrix} - \begin{bmatrix}
        9 & 3 & 12 \\
        3 & 1 & 4 \\
        12 & 4 & 16
        \end{bmatrix}) = \frac{1}{26} \begin{bmatrix}
            17 & -3 & -12 \\
            -3 & 25 & -4 \\
            -12 & -4 & 10 
    \end{bmatrix}
    \end{split}
\end{equation*}

\subsubsection*{(b)}
It is easy to find out that we need to find $\langle \begin {bmatrix}
    3 \\ 2 \\ 1
\end{bmatrix}\rangle ^ {\bot}$. 
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            3 \\
            2 \\
            1 
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            3 \\
            2 \\
            1  
        \end{bmatrix} (\begin{bmatrix}
            3 & 2 & 1 
        \end{bmatrix} \begin{bmatrix}
            3 \\
            2 \\
            1 
        \end{bmatrix})^{-1} \begin{bmatrix}
            3 & 2 & 1 
        \end{bmatrix} = \frac{1}{14}\begin{bmatrix}
                9 & 6 & 3 \\
                6 & 4 & 2 \\
                3 & 2 & 1
        \end{bmatrix} \\
        & [P_{U^{\bot}}]_\xi = I - [P_U]_\xi = \frac{1}{14}(\begin{bmatrix}
            14 & 0 & 0 \\
            0 & 14 & 0 \\
            0 & 0 & 14 
    \end{bmatrix} - \begin{bmatrix}
        9 & 6 & 3 \\
        6 & 4 & 2 \\
        3 & 2 & 1
        \end{bmatrix}) = \frac{1}{14} \begin{bmatrix}
            5 & -6 & -3 \\
            -6 & 10 & -2 \\
            -3 & -2 & 13
    \end{bmatrix}
    \end{split}
\end{equation*}

\subsubsection*{(c)}
It is easy to find out that we need to find $\langle \begin {bmatrix}
    1 \\ i \\ -i
\end{bmatrix}\rangle ^ {\bot}$. 
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            1 \\
            -i \\
            i 
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            1 \\
            -i \\
            i   
        \end{bmatrix} (\begin{bmatrix}
            1 & i & -i 
        \end{bmatrix} \begin{bmatrix}
            1 \\
            -i \\
            i 
        \end{bmatrix})^{-1} \begin{bmatrix}
            1 & i & -i 
        \end{bmatrix} = \frac{1}{3}\begin{bmatrix}
                1 & i & -i \\
                -i & 1 & -1 \\
                i & -1 & 1
        \end{bmatrix} \\
        & [P_{U^{\bot}}]_\xi = I - [P_U]_\xi = \frac{1}{3}(\begin{bmatrix}
            3 & 0 & 0 \\
            0 & 3 & 0 \\
            0 & 0 & 3 
    \end{bmatrix} - \begin{bmatrix}
        1 & i & -i \\
        -i & 1 & -1 \\
        i & -1 & 1
        \end{bmatrix}) = \frac{1}{3}\begin{bmatrix}
            2 & -i & i \\
            i & 2 & 1 \\
            -i & 1 & 2
    \end{bmatrix}
    \end{split}
\end{equation*}

\subsubsection*{(d)}
Do REF to the given matrix, we get $\begin{bmatrix}
    1 & 0 & 2 \\
    0 & 1 & -1 \\
    0 & 0 & 0 \\
    0 & 0 & 0 
\end{bmatrix}$. Which means that we only get first and second cols as basis.
\begin{equation*}
    \begin{split}
        &A = \begin{bmatrix}
            1 & 0 \\
            -1 & 3 \\
            0 & 2 \\
            2 & 1
        \end{bmatrix}\\
        &[P_U]_\xi = A(A^*A)^{-1}A^* = \begin{bmatrix}
            1 & 0 \\
            -1 & 3 \\
            0 & 2 \\
            2 & 1
        \end{bmatrix} (\begin{bmatrix}
            1 & -1 & 0 & 2 \\
            0 & 3 & 2 & 1 
        \end{bmatrix} \begin{bmatrix}
            1 & 0 \\
            -1 & 3 \\
            0 & 2 \\
            2 & 1
        \end{bmatrix})^{-1} \begin{bmatrix}
            1 & -1 & 0 & 2 \\
            0 & 3 & 2 & 1  
        \end{bmatrix} = \frac{1}{83}\begin{bmatrix}
                14 & -11 & 2 & 29 \\
                -11 & 62 & 34 & -5 \\
                2 & 34 & 24 & 16 \\
                29 & -5 & 16 & 66
        \end{bmatrix} 
    \end{split}
\end{equation*}

\subsection*{4.3.14}
\subsubsection*{(a)}
\begin{proof}
    Refer to \textbf{Theorem 1.10}
    Define $0$ is zero matrix in $\mathbb{V, W}$. $\forall v, v_1, v_2, w , w_1, w_2\in \mathbb{V, W}$, $c \in \mathbb{F}$we have
    \begin{equation*}
        \begin{split}
            &0 + v = v, 0^T = 0 \Rightarrow 0 \in \mathbb{V}\\
            &0 + w = w, 0^T = -0 \Rightarrow 0 \in \mathbb{W} \\
            & v_1 + cv_2 = (v_1 + cv_2)^T \Rightarrow v_1 + cv_2 \in \mathbb{V} \\
            & w_1 + cw_2 = (-w_1 - cw_2)^T = -(w_1 + cw_2)^T\Rightarrow w_1 + cw_2 \in \mathbb{W} 
        \end{split}
    \end{equation*}
    So $\mathbb{V, W}$ is subspace of $M_n(\mathbb{R})$.
\end{proof}

\subsubsection*{(b)}
\noindent   Refer to \textbf{Frobenius Inner Product}: $ \forall X, Y \in M_n(\mathbb{R}), X^T = X^*, \langle X, Y \rangle = tr(XY^*) = tr(XY^T)$, since the field we are using here is $\mathbb{R}$.
    \begin{claim}
        Rewrite V as $V = \{ \frac {X + X ^ T} {2} | X \in M_n(\mathbb{R}) \}$, W as $W = \{ \frac {X - X ^ T} {2} | X \in M_n(\mathbb{R}) \}$.
        $(\frac {X + X ^ T} {2})^T = \frac {X^T + X} {2} = \frac {X + X^T} {2}$, and $(\frac {X - X ^ T} {2})^T = \frac {X^T - X} {2} = - \frac {X - X^T} {2}$
    \end{claim}
    By the \textbf{Definition Of Orthogonal Projection}, I can build a function $P_V: M_n(\mathbb{R}) \rightarrow M_n(\mathbb{R})$ 
    and $X = \frac {X + X ^ T} {2} + \frac {X - X ^ T} {2}, X \in M_n(\mathbb{R}), \frac{X + X ^ T} {2} \in \mathbb{V}, \frac {X - X ^ T} {2} \in \mathbb{W}, P_V(X) = 
    \frac {X + X ^ T} {2}$.
\begin{proof}
    By the \textbf{Proposition 3.60}
    $\forall v \in \mathbb{V}, \forall w \in \mathbb{W}$, $tr(vw^T) = -tr(vw) = -tr(v^Tw) = - tr(wv^T) = - tr(wv^T)^T = - tr(vw^T) \Rightarrow tr(vw^T) = 0$.
    Therfore the function I build is valid. $\mathbb{V^\bot} = \mathbb{W}$.
\end{proof}
 
\subsubsection*{(c)}

\noindent Refer to the conclusion from \textbf{(b)}: $P_V(X) = \frac {X + X ^ T} {2} = \frac {X + X ^ *} {2} = Re(A)$,  
$P_W(X) (= P_{V^\bot}(X) = I(X) - P_V(X)) = \frac {X - X ^ T} {2} = \frac {X - X ^ *} {2} = i \frac {X - X ^ *} {2i} = iIm(A)$

\subsection*{4.3.22}
\subsubsection*{(a)}
\begin{proof}
    $\forall i \in \{1, 2, ..., n \}$, the $ith$ col of $[P_U]_B$ is $\begin{bmatrix}
        \langle P_U(e_i), e_1 \rangle \\ \langle P_U(e_i), e_2 \rangle \\ ... \\ \langle P_U(e_i), e_n \rangle
    \end{bmatrix}$. \\
    When $i \le m$, $\begin{bmatrix}
        \langle P_U(e_i), e_1 \rangle \\ \langle P_U(e_i), e_2 \rangle \\ ... \\ \langle P_U(e_i), e_n \rangle
    \end{bmatrix} = \begin{bmatrix}
        \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_1 \rangle \\ \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_2 \rangle \\ ... \\ \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_n \rangle
    \end{bmatrix} = \begin{bmatrix}
        \langle e_i, e_1 \rangle \\ \langle e_i, e_2 \rangle \\ ... \\ \langle e_i, e_n \rangle
    \end{bmatrix}= \begin{bmatrix}
        0 \\ 0 \\ ... \\1 \\ ...\\ 0
    \end{bmatrix}$. \\ The $ith$ row will be one which is $[[P_U]_B]_{ii} = 1$. \\
    When $i > m$, $\begin{bmatrix}
        \langle P_U(e_i), e_1 \rangle \\ \langle P_U(e_i), e_2 \rangle \\ ... \\ \langle P_U(e_i), e_n \rangle
    \end{bmatrix} = \begin{bmatrix}
        \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_1 \rangle \\ \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_2 \rangle \\ ... \\ \langle \sum_{k = 1}^{k = m} \langle e_i,e_k \rangle e_k  , e_n \rangle
    \end{bmatrix} = \begin{bmatrix}
        \langle 0, e_1 \rangle \\ \langle 0, e_2 \rangle \\ ... \\ \langle 0, e_n \rangle
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 0 \\ ... \\0 \\ ...\\ 0
    \end{bmatrix}$. \\
    $\Rightarrow [P_U]_B = diag(1, ... 1, 0, ..., 0)$.
\end{proof}

\subsubsection*{(b)}
\begin{proof}
    Refer to the \textbf{Definition Of Orthogonal Projection}: $\forall u \in rangeP_U, u \in U \\ \Rightarrow RangeP_U \subset U$.\\
    By the conclusion of \textbf{part2}, we know $ \forall u \in U, u = \sum _{j = 1}^{j = m} a_je_j, P_U(u) = \sum _{i = 1}^{i = m} \langle u, e_i \rangle e_i = 
    \sum _{i = 1}^{i = m} \langle \sum_{j = 1}^{j = m} a_je_j, e_i \rangle e_i  = \sum _{i = 1}^{i = m} \sum_{j = 1}^{j = m} a_j \langle e_j, e_i \rangle e_i
    = \sum _{i = 1}^{i = m} a_ie_i = u \Rightarrow \forall u \in U, P_U(u) = u$. I can conclude that $\forall u \in U, \exists u \in U \subset V, u = P_U(u) \in
    RangeP_U \Rightarrow U \subset RangeP_U$.\\
    $ \Rightarrow U = RangeP_U$
\end{proof}

\subsubsection*{(c)}
\begin{proof}
    Refer to \textbf{Definition Of Orthogonal Projection}: $\forall v \in V, u \in U, w \in U^\bot, v = u + w, P_U(v) = u$. \\
    $\forall v \in KerP_U, P_U(v) = 0 \Rightarrow v = 0 + v \Rightarrow v \in U^\bot \Rightarrow KerP_U \subset U^\bot$ \\
    $\forall v \in U^\bot \subset V, v = 0 +v \Rightarrow P_U(v) = 0 \Rightarrow v \in KerU^\bot \Rightarrow U^\bot \subset KerP_U$ \\
    $\Rightarrow U^\bot = KerP_U$
\end{proof}

\subsubsection*{(d)}
\begin{proof}
    Suppose $\forall v \in V, v = u + w, u \in U, w \in U^\bot$
    \begin{equation*}
        (I - P_U)v = v - P_U(v) = v - u = w = P_{U^\bot}(v) \Rightarrow I - P_U = P_{U^\bot}
    \end{equation*}
\end{proof}

\subsubsection*{(e)}
\begin{proof}
    Refer to the conclusion from \textbf{(b)}
    \begin{equation*}
        \begin{split}
            \forall v \in V, u \in U, w \in U^{\bot}, v = u + w, P^2_U(v) = P_U(u) = u = P_U(v) \Rightarrow P^2_U = P_U
        \end{split}
    \end{equation*}
\end{proof}





\end{document}
